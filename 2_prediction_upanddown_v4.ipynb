{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modifications:\n",
    "\n",
    "1. Turn different models into another file, and VWAP should import the models to read models.\n",
    "2. Rewrite VWAP LSTM factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Activation\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import keras\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "data_dir=\"./data/\"\n",
    "ticker=\"TSLA\"\n",
    "\n",
    "date_pool=pd.date_range(\"1/1/2019\",\"1/31/2019\",freq=\"B\").strftime(\"%Y%m%d\")\n",
    "date_pool=[d for d in date_pool if os.path.exists(data_dir+\"trades_{}_{}.csv\".format(d,ticker))]\n",
    "\n",
    "train_days=10\n",
    "train_date_list=date_pool[:train_days]\n",
    "test_date_list=date_pool[train_days+1:]\n",
    "time_steps = 50\n",
    "\n",
    "nforward=10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pipeline():\n",
    "    def __init__(self):\n",
    "        self.x = None\n",
    "        self.y = None\n",
    "        self.scaler = None\n",
    "    def load_data(self, ticker, date):\n",
    "        df = pd.read_csv(data_dir+'trades_{}_{}.csv'.format(date, ticker),index_col=[0],parse_dates=[0])\n",
    "\n",
    "        # Feature Engineering\n",
    "        df[\"direction\"]=(df[\"trade_px\"]-df[\"trade_px\"].shift(1)).apply(np.sign)\n",
    "        df[\"pct_change\"]=df[\"trade_px\"].pct_change()\n",
    "\n",
    "        mysign=lambda x: 0 if abs(x)<1e-3 else (1 if x>0 else -1)\n",
    "        df[\"label\"]=(df[\"trade_px\"].rolling(nforward).mean().shift(-nforward)-df[\"trade_px\"]).apply(mysign)\n",
    "        # df[\"label\"]=(df[\"trade_px\"].shift(-1)-df[\"trade_px\"]).apply(np.sign) # last version\n",
    "\n",
    "        df.fillna(method=\"ffill\",inplace=True)\n",
    "        df.dropna(axis=0,inplace=True)\n",
    "        # print(df.head(10),df.shape)\n",
    "        # print(\"NaN number: \",df.isna().sum().sum())\n",
    "        \n",
    "        \n",
    "        # trade_px removed\n",
    "        return df[[\"trade_size\",\"pct_change\",\"direction\",\"label\"]].values\n",
    "\n",
    "    def create_dataset(self, ticker=ticker, dates=train_date_list, time_steps = time_steps, input_scaler=None):  \n",
    "        for i,d in enumerate(dates):\n",
    "            datanew = self.load_data(ticker,d)\n",
    "            if i==0:\n",
    "                data=datanew\n",
    "            else:\n",
    "                data=np.vstack((data, datanew))\n",
    "        \n",
    "        label=data[:,-1]\n",
    "        data=data[:,:-1]\n",
    "\n",
    "        if input_scaler is None:\n",
    "            scaler=StandardScaler()\n",
    "            data=scaler.fit_transform(data)\n",
    "        else:\n",
    "            data=input_scaler.transform(data)\n",
    "            scaler=input_scaler\n",
    "\n",
    "        x = [data[0 : time_steps]]\n",
    "        y = [label[time_steps-1]]\n",
    "        N=len(data)//time_steps\n",
    "\n",
    "        print(N)\n",
    "        for i in range(1, N):\n",
    "            t = data[i*time_steps: (i + 1)*time_steps]\n",
    "            x = np.vstack((x, [t]))\n",
    "            y.append(label[(i + 1)*time_steps-1])\n",
    "\n",
    "        y=pd.get_dummies(y)\n",
    "        #print(y)\n",
    "\n",
    "        return x,y.values,scaler\n",
    "\n",
    "    def loss_plot(self, history, plot_name = 'Loss'): # type(history) is dict\n",
    "        loss = np.asarray(history['loss'])\n",
    "        val_loss = np.asarray(history['val_loss'])\n",
    "\n",
    "        plt.style.use('seaborn')\n",
    "        plt.figure(figsize = (20,6), dpi=dpi)\n",
    "        plt.grid(True)\n",
    "        plt.plot(loss, color = 'darkgrey')\n",
    "        plt.plot(val_loss, color = 'tan')\n",
    "        plt.legend(['loss', 'val_loss'])\n",
    "        # plt.savefig('{}_{}_{}_{}_{}.png'.format(ticker, plot_name, str(n_epochs), str(time_steps), str(batch_size)))\n",
    "    \n",
    "    \n",
    "    def training_data_transform(self, ticker):\n",
    "        # Load train data\n",
    "        x, y, scaler = self.create_dataset(ticker)\n",
    "        self.x, self.y, self.scaler = x, y, scaler\n",
    "        print(\"Finished loading data.\")\n",
    "\n",
    "        with open(\"model/scaler_{}_{}.p\".format(train_date_list[0],train_date_list[-1]),\"wb\") as f:\n",
    "            pickle.dump(scaler,f)\n",
    "\n",
    "    def model_training_testing(self, ticker, model, plot = False):\n",
    "        # Model Training pipeline\n",
    "        model_functionalities = Model_Functionalities(model)\n",
    "        \n",
    "        x, y, scaler = self.x, self.y, self.scaler\n",
    "        print(y[:10])\n",
    "        \n",
    "        if x is None:\n",
    "            print(\"None Training data processed\")\n",
    "            return\n",
    "        \n",
    "        # Build model, in-sample train test\n",
    "        train_history = model_functionalities.train_test(x, y, plot)  \n",
    "        if model_functionalities.model.model_name == \"LSTM\":\n",
    "            if plot == True:\n",
    "                self.loss_plot(train_history.history)\n",
    "\n",
    "        with open(\"model/\" + model.model_name + \"_{}_{}.p\".format(train_date_list[0],train_date_list[-1]),\"wb\") as f:\n",
    "            pickle.dump(model_functionalities.model,f)\n",
    "\n",
    "        # Out-of-sample test\n",
    "        for test_date in test_date_list:\n",
    "            # create test dateset\n",
    "            x_test, y_test, _ = self.create_dataset(ticker=ticker, dates=[test_date], time_steps = time_steps, input_scaler=scaler)\n",
    "            x_test, y_test = model_functionalities.model.reshape_dataset(x_test, y_test)\n",
    "\n",
    "            # use precious trained model to test\n",
    "            y_test_pred = model_functionalities.predict(x_test)\n",
    "            if model_functionalities.model.model_name == \"LSTM\":\n",
    "                if plot == True:\n",
    "                    model_functionalities.view_accuracy(y_test_pred.argmax(axis=1), y_test.argmax(axis=1))\n",
    "            if y_test.shape[1] != 1:\n",
    "                accuracy = np.mean(y_test_pred.argmax(axis=1)==y_test.argmax(axis=1))\n",
    "            else:\n",
    "                accuracy = np.mean(y_test_pred==y_test)\n",
    "            print(test_date+\" accuracy: \", accuracy)\n",
    "        return model_functionalities.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model_Functionalities():\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "    def train_test(self, x, y, plot = False):\n",
    "        \n",
    "        size = len(x)\n",
    "        if size!=len(y):\n",
    "            return None\n",
    "        x = x[: batch_size * (size // batch_size)]\n",
    "        y = y[: batch_size * (size // batch_size)]\n",
    "        \n",
    "        x, y = self.model.reshape_dataset(x, y)\n",
    "\n",
    "        x_train, x_validation, y_train, y_validation= train_test_split(x, y, test_size = 0.1, shuffle = True)\n",
    "        print('train', x_train.shape, y_train.shape)\n",
    "        print('validation', x_validation.shape, y_validation.shape)\n",
    "        \n",
    "        \n",
    "        if self.model.model_name == \"LSTM\" or  self.model.model_name == \"CNN\":\n",
    "            early_stopping = EarlyStopping(monitor='val_loss', patience=stop_patience, mode=\"min\", verbose=2, restore_best_weights=True)\n",
    "            history = self.model.fit(x_train, y_train, batch_size = batch_size, epochs = n_epochs,\n",
    "                                     validation_data=(x_validation, y_validation),callbacks=[early_stopping])\n",
    "        else:\n",
    "            self.model.fit(x_train, y_train)\n",
    "        \n",
    "        self.y_pred = self.model.predict(x_validation)\n",
    "        self.y_validation_true = y_validation\n",
    "        \n",
    "        if plot == True:\n",
    "            if y.shape[1] != 1:\n",
    "                self.train_plot = self.view_accuracy(self.predict(x_train).argmax(axis=1), y_train.argmax(axis=1), 'Train')\n",
    "                self.validation_plot = self.view_accuracy(self.predict(x_validation).argmax(axis=1), y_validation.argmax(axis=1), 'Validation')\n",
    "            else:\n",
    "                self.train_plot = self.view_accuracy(self.predict(x_train), y_train, 'Train')\n",
    "                self.validation_plot = self.view_accuracy(self.predict(x_validation), y_validation, 'Validation')\n",
    "        if self.model.model_name == \"LSTM\":\n",
    "            return history\n",
    "\n",
    "    def predict(self, x_validation):\n",
    "        pred = self.model.predict(x_validation)\n",
    "        return pred\n",
    "    \n",
    "    def view_accuracy(self, y_pred = None, y_true = None, plot_name = 'Test', num=100):\n",
    "        if y_pred is None:\n",
    "            y_pred = self.y_pred.argmax(axis=1)\n",
    "            y_true = self.y_validation_true.argmax(axis=1)\n",
    "        \n",
    "        plt.style.use('seaborn')\n",
    "        plt.figure(figsize = (20,6), dpi=dpi)\n",
    "        plt.grid(True)\n",
    "        plt.plot(y_pred[:num], color = 'lightcoral')\n",
    "        plt.plot(y_true[:num], color = 'cornflowerblue', linewidth = 1)\n",
    "        plt.title('{}_{}'.format(ticker, plot_name))\n",
    "        plt.legend(['predict', 'true'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15154\n",
      "Finished loading data.\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline()\n",
    "pipeline.training_data_transform(ticker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive 0.4660815626237297\n",
      "neutral 0.057146627953015706\n",
      "negative 0.4767718094232546\n"
     ]
    }
   ],
   "source": [
    "labels = pipeline.y.argmax(axis = 1) - 1\n",
    "print(\"positive\", (labels == 1).mean())\n",
    "print(\"neutral\", (labels == 0).mean())\n",
    "print(\"negative\", (labels == -1).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using LSTM to predict next up and down\n",
    "## v2: Predict up and downs of the average of $\\mathbf{nforward}=10$ following prices\n",
    "\n",
    "1. Too slow to predict (next trade may happen in millisecond)\n",
    "2. Only classification of up, down and same. No quantitative prediction (can be improved to predict quantity of price movement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_dim = 30\n",
    "n_epochs = 100\n",
    "batch_size = 128\n",
    "activation = \"tanh\"\n",
    "loss = 'categorical_crossentropy'\n",
    "stop_patience=20\n",
    "\n",
    "dpi=200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 0]\n",
      " [0 0 1]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [0 0 1]\n",
      " [1 0 0]\n",
      " [0 1 0]\n",
      " [1 0 0]\n",
      " [0 0 1]\n",
      " [0 0 1]]\n",
      "train (13593, 50, 3) (13593, 3)\n",
      "validation (1511, 50, 3) (1511, 3)\n",
      "Train on 13593 samples, validate on 1511 samples\n",
      "Epoch 1/100\n",
      "13593/13593 [==============================] - 10s 742us/step - loss: 0.8859 - accuracy: 0.5092 - val_loss: 0.8244 - val_accuracy: 0.5480\n",
      "Epoch 2/100\n",
      "13593/13593 [==============================] - 9s 648us/step - loss: 0.8165 - accuracy: 0.5648 - val_loss: 0.7988 - val_accuracy: 0.5612\n",
      "Epoch 3/100\n",
      "13593/13593 [==============================] - 9s 649us/step - loss: 0.8007 - accuracy: 0.5759 - val_loss: 0.7956 - val_accuracy: 0.5645\n",
      "Epoch 4/100\n",
      "13593/13593 [==============================] - 9s 641us/step - loss: 0.7961 - accuracy: 0.5751 - val_loss: 0.7913 - val_accuracy: 0.5632\n",
      "Epoch 5/100\n",
      "13593/13593 [==============================] - 9s 646us/step - loss: 0.7928 - accuracy: 0.5824 - val_loss: 0.7885 - val_accuracy: 0.5692\n",
      "Epoch 6/100\n",
      "13593/13593 [==============================] - 9s 639us/step - loss: 0.7908 - accuracy: 0.5798 - val_loss: 0.7887 - val_accuracy: 0.5619\n",
      "Epoch 7/100\n",
      "13593/13593 [==============================] - 9s 649us/step - loss: 0.7879 - accuracy: 0.5877 - val_loss: 0.7848 - val_accuracy: 0.5659\n",
      "Epoch 8/100\n",
      "13593/13593 [==============================] - 9s 644us/step - loss: 0.7864 - accuracy: 0.5830 - val_loss: 0.7846 - val_accuracy: 0.5698\n",
      "Epoch 9/100\n",
      "13593/13593 [==============================] - 9s 638us/step - loss: 0.7840 - accuracy: 0.5815 - val_loss: 0.7836 - val_accuracy: 0.5745\n",
      "Epoch 10/100\n",
      "13593/13593 [==============================] - 9s 652us/step - loss: 0.7830 - accuracy: 0.5829 - val_loss: 0.7823 - val_accuracy: 0.5705\n",
      "Epoch 11/100\n",
      "13593/13593 [==============================] - 9s 652us/step - loss: 0.7809 - accuracy: 0.5872 - val_loss: 0.7841 - val_accuracy: 0.5738\n",
      "Epoch 12/100\n",
      "13593/13593 [==============================] - 9s 651us/step - loss: 0.7804 - accuracy: 0.5885 - val_loss: 0.7819 - val_accuracy: 0.5685\n",
      "Epoch 13/100\n",
      "13593/13593 [==============================] - 9s 649us/step - loss: 0.7781 - accuracy: 0.5874 - val_loss: 0.7856 - val_accuracy: 0.5665\n",
      "Epoch 14/100\n",
      "13593/13593 [==============================] - 9s 653us/step - loss: 0.7780 - accuracy: 0.5875 - val_loss: 0.7817 - val_accuracy: 0.5725\n",
      "Epoch 15/100\n",
      "13593/13593 [==============================] - 9s 651us/step - loss: 0.7770 - accuracy: 0.5895 - val_loss: 0.7865 - val_accuracy: 0.5731\n",
      "Epoch 16/100\n",
      "13593/13593 [==============================] - 9s 648us/step - loss: 0.7769 - accuracy: 0.5893 - val_loss: 0.7845 - val_accuracy: 0.5652\n",
      "Epoch 17/100\n",
      "13593/13593 [==============================] - 9s 649us/step - loss: 0.7761 - accuracy: 0.5928 - val_loss: 0.7826 - val_accuracy: 0.5698\n",
      "Epoch 18/100\n",
      "13593/13593 [==============================] - 9s 627us/step - loss: 0.7749 - accuracy: 0.5903 - val_loss: 0.7859 - val_accuracy: 0.5659\n",
      "Epoch 19/100\n",
      "13593/13593 [==============================] - 9s 640us/step - loss: 0.7740 - accuracy: 0.5935 - val_loss: 0.7882 - val_accuracy: 0.5698\n",
      "Epoch 20/100\n",
      "13593/13593 [==============================] - 9s 647us/step - loss: 0.7738 - accuracy: 0.5909 - val_loss: 0.7836 - val_accuracy: 0.5698\n",
      "Epoch 21/100\n",
      "13593/13593 [==============================] - 9s 654us/step - loss: 0.7736 - accuracy: 0.5927 - val_loss: 0.7833 - val_accuracy: 0.5685\n",
      "Epoch 22/100\n",
      "13593/13593 [==============================] - 9s 660us/step - loss: 0.7719 - accuracy: 0.5926 - val_loss: 0.7828 - val_accuracy: 0.5738\n",
      "Epoch 23/100\n",
      "13593/13593 [==============================] - 9s 654us/step - loss: 0.7712 - accuracy: 0.5906 - val_loss: 0.7828 - val_accuracy: 0.5758\n",
      "Epoch 24/100\n",
      "13593/13593 [==============================] - 9s 648us/step - loss: 0.7701 - accuracy: 0.5943 - val_loss: 0.7835 - val_accuracy: 0.5745\n",
      "Epoch 25/100\n",
      "13593/13593 [==============================] - 9s 661us/step - loss: 0.7700 - accuracy: 0.5936 - val_loss: 0.7855 - val_accuracy: 0.5771\n",
      "Epoch 26/100\n",
      "13593/13593 [==============================] - 9s 644us/step - loss: 0.7709 - accuracy: 0.5908 - val_loss: 0.7839 - val_accuracy: 0.5672\n",
      "Epoch 27/100\n",
      "13593/13593 [==============================] - 9s 625us/step - loss: 0.7688 - accuracy: 0.5935 - val_loss: 0.7874 - val_accuracy: 0.5698\n",
      "Epoch 28/100\n",
      "13593/13593 [==============================] - 9s 631us/step - loss: 0.7686 - accuracy: 0.5927 - val_loss: 0.7851 - val_accuracy: 0.5771\n",
      "Epoch 29/100\n",
      "13593/13593 [==============================] - 9s 650us/step - loss: 0.7670 - accuracy: 0.5957 - val_loss: 0.7865 - val_accuracy: 0.5731\n",
      "Epoch 30/100\n",
      "13593/13593 [==============================] - 9s 651us/step - loss: 0.7680 - accuracy: 0.5916 - val_loss: 0.7835 - val_accuracy: 0.5745\n",
      "Epoch 31/100\n",
      "13593/13593 [==============================] - 9s 650us/step - loss: 0.7659 - accuracy: 0.5947 - val_loss: 0.7867 - val_accuracy: 0.5745\n",
      "Epoch 32/100\n",
      "13593/13593 [==============================] - 9s 647us/step - loss: 0.7656 - accuracy: 0.5981 - val_loss: 0.7899 - val_accuracy: 0.5685\n",
      "Epoch 33/100\n",
      "13593/13593 [==============================] - 9s 656us/step - loss: 0.7652 - accuracy: 0.5967 - val_loss: 0.7869 - val_accuracy: 0.5811\n",
      "Epoch 34/100\n",
      "13593/13593 [==============================] - 9s 664us/step - loss: 0.7644 - accuracy: 0.5960 - val_loss: 0.7876 - val_accuracy: 0.5731\n",
      "Restoring model weights from the end of the best epoch\n",
      "Epoch 00034: early stopping\n",
      "808\n",
      "20190117 accuracy:  0.5717821782178217\n",
      "5342\n",
      "20190118 accuracy:  0.6169973792587046\n",
      "2570\n",
      "20190122 accuracy:  0.6046692607003891\n",
      "2944\n",
      "20190123 accuracy:  0.6324728260869565\n",
      "1759\n",
      "20190124 accuracy:  0.6043206367254121\n",
      "1610\n",
      "20190125 accuracy:  0.593167701863354\n",
      "1428\n",
      "20190128 accuracy:  0.5819327731092437\n",
      "1034\n",
      "20190129 accuracy:  0.5851063829787234\n",
      "2164\n",
      "20190130 accuracy:  0.5697781885397413\n",
      "2679\n",
      "20190131 accuracy:  0.6151549085479656\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<models.LSTM_Model at 0x7f19d2a477d0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_model = LSTM_Model(time_steps = time_steps,  hidden_dim = hidden_dim, n_epochs = n_epochs,\n",
    "                        activation = activation, loss = loss, \n",
    "                        data_dim = pipeline.x.shape[-1], output_dim = pipeline.y.shape[-1])\n",
    "lstm_model.build()\n",
    "pipeline.model_training_testing(ticker, lstm_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15154, 50, 3) (15154, 3)\n"
     ]
    }
   ],
   "source": [
    "print(pipeline.x.shape, pipeline.y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Conv2D, Flatten, Dropout\n",
    "from keras import regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_dim = 30\n",
    "n_epochs = 100\n",
    "batch_size = 128\n",
    "activation = \"relu\"\n",
    "loss = 'categorical_crossentropy'\n",
    "stop_patience=20\n",
    "\n",
    "dpi=200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 0]\n",
      " [0 0 1]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [0 0 1]\n",
      " [1 0 0]\n",
      " [0 1 0]\n",
      " [1 0 0]\n",
      " [0 0 1]\n",
      " [0 0 1]]\n",
      "train (13593, 50, 3, 1) (13593, 3)\n",
      "validation (1511, 50, 3, 1) (1511, 3)\n",
      "Train on 13593 samples, validate on 1511 samples\n",
      "Epoch 1/100\n",
      "13593/13593 [==============================] - 6s 409us/step - loss: 0.8656 - accuracy: 0.5373 - val_loss: 0.8400 - val_accuracy: 0.5414\n",
      "Epoch 2/100\n",
      "13593/13593 [==============================] - 5s 382us/step - loss: 0.8178 - accuracy: 0.5659 - val_loss: 0.8231 - val_accuracy: 0.5612\n",
      "Epoch 3/100\n",
      "13593/13593 [==============================] - 5s 390us/step - loss: 0.8006 - accuracy: 0.5802 - val_loss: 0.8189 - val_accuracy: 0.5619\n",
      "Epoch 4/100\n",
      "13593/13593 [==============================] - 5s 391us/step - loss: 0.7887 - accuracy: 0.5932 - val_loss: 0.8148 - val_accuracy: 0.5797\n",
      "Epoch 5/100\n",
      "13593/13593 [==============================] - 5s 389us/step - loss: 0.7787 - accuracy: 0.5949 - val_loss: 0.8302 - val_accuracy: 0.5652\n",
      "Epoch 6/100\n",
      "13593/13593 [==============================] - 5s 390us/step - loss: 0.7740 - accuracy: 0.6102 - val_loss: 0.8329 - val_accuracy: 0.5467\n",
      "Epoch 7/100\n",
      "13593/13593 [==============================] - 5s 401us/step - loss: 0.7698 - accuracy: 0.6108 - val_loss: 0.8326 - val_accuracy: 0.5685\n",
      "Epoch 8/100\n",
      "13593/13593 [==============================] - 5s 402us/step - loss: 0.7657 - accuracy: 0.6109 - val_loss: 0.8472 - val_accuracy: 0.5705\n",
      "Epoch 9/100\n",
      "13593/13593 [==============================] - 6s 406us/step - loss: 0.7620 - accuracy: 0.6187 - val_loss: 0.8397 - val_accuracy: 0.5486\n",
      "Epoch 10/100\n",
      "13593/13593 [==============================] - 5s 401us/step - loss: 0.7536 - accuracy: 0.6294 - val_loss: 0.8557 - val_accuracy: 0.5453\n",
      "Epoch 11/100\n",
      "13593/13593 [==============================] - 5s 399us/step - loss: 0.7517 - accuracy: 0.6251 - val_loss: 0.8556 - val_accuracy: 0.5612\n",
      "Epoch 12/100\n",
      "13593/13593 [==============================] - 5s 399us/step - loss: 0.7467 - accuracy: 0.6329 - val_loss: 0.8597 - val_accuracy: 0.5619\n",
      "Epoch 13/100\n",
      "13593/13593 [==============================] - 5s 396us/step - loss: 0.7398 - accuracy: 0.6375 - val_loss: 0.8620 - val_accuracy: 0.5566\n",
      "Epoch 14/100\n",
      "13593/13593 [==============================] - 5s 397us/step - loss: 0.7339 - accuracy: 0.6456 - val_loss: 0.8797 - val_accuracy: 0.5586\n",
      "Epoch 15/100\n",
      "13593/13593 [==============================] - 5s 387us/step - loss: 0.7274 - accuracy: 0.6534 - val_loss: 0.8960 - val_accuracy: 0.5460\n",
      "Epoch 16/100\n",
      "13593/13593 [==============================] - 5s 384us/step - loss: 0.7254 - accuracy: 0.6527 - val_loss: 0.8948 - val_accuracy: 0.5467\n",
      "Epoch 17/100\n",
      "13593/13593 [==============================] - 5s 398us/step - loss: 0.7190 - accuracy: 0.6595 - val_loss: 0.8957 - val_accuracy: 0.5533\n",
      "Epoch 18/100\n",
      "13593/13593 [==============================] - 5s 400us/step - loss: 0.7101 - accuracy: 0.6635 - val_loss: 0.9026 - val_accuracy: 0.5533\n",
      "Epoch 19/100\n",
      "13593/13593 [==============================] - 5s 402us/step - loss: 0.7011 - accuracy: 0.6710 - val_loss: 0.9124 - val_accuracy: 0.5520\n",
      "Epoch 20/100\n",
      "13593/13593 [==============================] - 5s 392us/step - loss: 0.6930 - accuracy: 0.6740 - val_loss: 0.9262 - val_accuracy: 0.5539\n",
      "Epoch 21/100\n",
      "13593/13593 [==============================] - 5s 391us/step - loss: 0.6876 - accuracy: 0.6798 - val_loss: 0.9453 - val_accuracy: 0.5341\n",
      "Epoch 22/100\n",
      "13593/13593 [==============================] - 5s 399us/step - loss: 0.6808 - accuracy: 0.6854 - val_loss: 0.9532 - val_accuracy: 0.5493\n",
      "Epoch 23/100\n",
      "13593/13593 [==============================] - 5s 404us/step - loss: 0.6752 - accuracy: 0.6910 - val_loss: 0.9602 - val_accuracy: 0.5281\n",
      "Epoch 24/100\n",
      "13593/13593 [==============================] - 5s 403us/step - loss: 0.6800 - accuracy: 0.6908 - val_loss: 0.9709 - val_accuracy: 0.5533\n",
      "Restoring model weights from the end of the best epoch\n",
      "Epoch 00024: early stopping\n",
      "808\n",
      "20190117 accuracy:  0.573019801980198\n",
      "5342\n",
      "20190118 accuracy:  0.6166229876450767\n",
      "2570\n",
      "20190122 accuracy:  0.6077821011673151\n",
      "2944\n",
      "20190123 accuracy:  0.6120923913043478\n",
      "1759\n",
      "20190124 accuracy:  0.5866969869243889\n",
      "1610\n",
      "20190125 accuracy:  0.5807453416149069\n",
      "1428\n",
      "20190128 accuracy:  0.5770308123249299\n",
      "1034\n",
      "20190129 accuracy:  0.5822050290135397\n",
      "2164\n",
      "20190130 accuracy:  0.5670055452865065\n",
      "2679\n",
      "20190131 accuracy:  0.5905188503172826\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<models.CNN_Model at 0x7f19bc2058d0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn_model = CNN_Model(time_steps = time_steps, hidden_dim = hidden_dim, n_epochs=n_epochs,\n",
    "                      activation = activation, loss = loss,\n",
    "                      data_dim = pipeline.x.shape[-1], output_dim = pipeline.y.shape[-1])\n",
    "cnn_model.build()\n",
    "pipeline.model_training_testing(ticker, cnn_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_depth=30\n",
    "n_estimators=100\n",
    "max_features=0.3\n",
    "criterion = 'gini'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 0]\n",
      " [0 0 1]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [0 0 1]\n",
      " [1 0 0]\n",
      " [0 1 0]\n",
      " [1 0 0]\n",
      " [0 0 1]\n",
      " [0 0 1]]\n",
      "train (13593, 150) (13593, 3)\n",
      "validation (1511, 150) (1511, 3)\n",
      "808\n",
      "20190117 accuracy:  0.5693069306930693\n",
      "5342\n",
      "20190118 accuracy:  0.5847997004867092\n",
      "2570\n",
      "20190122 accuracy:  0.5743190661478599\n",
      "2944\n",
      "20190123 accuracy:  0.5842391304347826\n",
      "1759\n",
      "20190124 accuracy:  0.5815804434337691\n",
      "1610\n",
      "20190125 accuracy:  0.5670807453416149\n",
      "1428\n",
      "20190128 accuracy:  0.5903361344537815\n",
      "1034\n",
      "20190129 accuracy:  0.5667311411992263\n",
      "2164\n",
      "20190130 accuracy:  0.5480591497227357\n",
      "2679\n",
      "20190131 accuracy:  0.555431131019037\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<models.RandomForest at 0x7f19bc01eb50>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_forest_model = RandomForest(max_depth, n_estimators, max_features, criterion)\n",
    "pipeline.model_training_testing(ticker, random_forest_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GradientBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators=30\n",
    "max_depth=30\n",
    "learning_rate=0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 0]\n",
      " [0 0 1]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [0 0 1]\n",
      " [1 0 0]\n",
      " [0 1 0]\n",
      " [1 0 0]\n",
      " [0 0 1]\n",
      " [0 0 1]]\n",
      "train (13593, 150) (13593, 1)\n",
      "validation (1511, 150) (1511, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/share/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/_gb.py:1454: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "808\n",
      "20190117 accuracy:  0.46700078423683955\n",
      "5342\n",
      "20190118 accuracy:  0.43066150975275436\n",
      "2570\n",
      "20190122 accuracy:  0.44654423231237417\n",
      "2944\n",
      "20190123 accuracy:  0.44813384721319705\n",
      "1759\n",
      "20190124 accuracy:  0.45929760727013935\n",
      "1610\n",
      "20190125 accuracy:  0.46287257436055707\n",
      "1428\n",
      "20190128 accuracy:  0.4647996453483354\n",
      "1034\n",
      "20190129 accuracy:  0.46313914901099557\n",
      "2164\n",
      "20190130 accuracy:  0.46121481237251477\n",
      "2679\n",
      "20190131 accuracy:  0.46323909811857006\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<models.GradientBoost at 0x7f199f5f5450>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradient_boost_model=GradientBoost(n_estimators=n_estimators,max_depth=max_depth,learning_rate=learning_rate)\n",
    "pipeline.model_training_testing(ticker, gradient_boost_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators=30\n",
    "max_depth=30\n",
    "learning_rate=0.05\n",
    "reg_lambda=0.1\n",
    "verbose=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 0]\n",
      " [0 0 1]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [0 0 1]\n",
      " [1 0 0]\n",
      " [0 1 0]\n",
      " [1 0 0]\n",
      " [0 0 1]\n",
      " [0 0 1]]\n",
      "train (13593, 150) (13593, 1)\n",
      "validation (1511, 150) (1511, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/share/anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/_label.py:235: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/usr/local/share/anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/_label.py:268: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "808\n",
      "20190117 accuracy:  0.4654629448093324\n",
      "5342\n",
      "20190118 accuracy:  0.42616828475516877\n",
      "2570\n",
      "20190122 accuracy:  0.4406003118896577\n",
      "2944\n",
      "20190123 accuracy:  0.4445616175862476\n",
      "1759\n",
      "20190124 accuracy:  0.45679508713572786\n",
      "1610\n",
      "20190125 accuracy:  0.46114656070367654\n",
      "1428\n",
      "20190128 accuracy:  0.4619627262669774\n",
      "1034\n",
      "20190129 accuracy:  0.4600600847771513\n",
      "2164\n",
      "20190130 accuracy:  0.4609771389328313\n",
      "2679\n",
      "20190131 accuracy:  0.4616508391132223\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<models.XGBoost at 0x7f199f5e5990>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgboost_model=XGBoost(n_estimators=n_estimators, max_depth=max_depth,learning_rate=learning_rate,reg_lambda=reg_lambda, verbose=verbose)\n",
    "pipeline.model_training_testing(ticker, xgboost_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Tick Factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of 20190117 0.4707643580036127\n",
      "Accuracy of 20190118 0.43095347731518396\n",
      "Accuracy of 20190122 0.4555151005928208\n",
      "Accuracy of 20190123 0.45868338302510847\n",
      "Accuracy of 20190124 0.4674517203359969\n",
      "Accuracy of 20190125 0.463677230280151\n",
      "Accuracy of 20190128 0.48354217710885544\n",
      "Accuracy of 20190129 0.45997873779839565\n",
      "Accuracy of 20190130 0.45219857893910137\n",
      "Accuracy of 20190131 0.4725264061508603\n"
     ]
    }
   ],
   "source": [
    "# TICK FACTOR\n",
    "# only update if it's a trade\n",
    "# if message_type == 't':\n",
    "#     # calc the tick\n",
    "#     this_tick = np.sign(last_price - prev_price)\n",
    "#     if this_tick == 0:\n",
    "#         this_tick = prev_tick\n",
    "\n",
    "#     # now calc the tick\n",
    "#     if tick_factor == 0:\n",
    "#         tick_factor = this_tick\n",
    "#     else:\n",
    "#         tick_factor = (tick_ema_alpha * this_tick) + (1 - tick_ema_alpha) * tick_factor\n",
    "\n",
    "#         # store the last tick\n",
    "#     prev_tick = this_tick\n",
    "\n",
    "for test_date in test_date_list:\n",
    "    df = pd.read_csv(data_dir+'trades_{}_{}.csv'.format(test_date, ticker),index_col=[0],parse_dates=[0])\n",
    "    \n",
    "    df[\"tick_test\"]=(df[\"trade_px\"]-df[\"trade_px\"].shift(1)).apply(lambda x: 1 if x>0. else (-1. if x<0 else np.nan))\n",
    "    df.fillna(method=\"ffill\",inplace=True)\n",
    "    \n",
    "    df[\"tick_factor\"]=df[\"tick_test\"].ewm(span=20).mean()\n",
    "    df.dropna(axis=0,inplace=True)\n",
    "    \n",
    "    mysign = lambda x: 0 if abs(x)<1e-5 else (1 if x>0 else -1)\n",
    "    df[\"predict\"]=df[\"tick_factor\"].apply(mysign)\n",
    "    df[\"real_movement\"]=(df[\"trade_px\"].rolling(nforward).mean().shift(-nforward)-df[\"trade_px\"]).apply(mysign)\n",
    "    \n",
    "    df.dropna(axis=0,inplace=True)\n",
    "    acc=np.mean(df[\"predict\"]==df[\"real_movement\"])\n",
    "    print(\"Accuracy of {}\".format(test_date),acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average time interval between 10 trades is 0 days 00:00:02.094658\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sum=pd.Timedelta(0)\n",
    "count=0\n",
    "for test_date in test_date_list:\n",
    "    df = pd.read_csv(data_dir+'trades_{}_{}.csv'.format(test_date, ticker),index_col=[0],parse_dates=[0])\n",
    "    timestamp=pd.DataFrame({\"trade_time\":df.index})\n",
    "    dt=timestamp[\"trade_time\"].shift(-nforward)-timestamp[\"trade_time\"]\n",
    "    dt.dropna(axis=0,inplace=True)\n",
    "    dt.apply(lambda x:x.seconds).hist()\n",
    "    sum+=dt.sum()\n",
    "    count+=dt.shape[0]\n",
    "print(\"Average time interval between {} trades is\".format(nforward),sum/count)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
